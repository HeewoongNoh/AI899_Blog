---
layout: distill
title: AI810 Blog Post (20255142)
description: This blog describes two papers on molecular generation method (1) Drug Discovery with Dynamic Goal-aware Fragments, accepted at ICML 2024, and (2) MAGNet Motif-Agnostic Generation of Molecules From Scaffolds, accepted as a spotlight paper at ICLR 2025.
htmlwidgets: true
date: 2025-04-28
hidden: false

# authors:
#   - name: Heewoong Noh
    # url: "https://en.wikipedia.org/wiki/Albert_Einstein"
    # affiliations:
    #   name: KAIST ISysE DSAIL

authors:
  - name: Heewoong Noh
    affiliations:
      name: KAIST ISysE DSAIL


# must be the exact same name as your blogpost
bibliography: 2025-04-28-20255142.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: "[Paper 1] Drug Discovery with Dynamic Goal-aware Fragments"
  - name: "[Paper 2] MAGNet: Motif-Agnostic Generation of Molecules From Scaffolds"

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---



## [Paper 1] Drug Discovery with Dynamic Goal-aware Fragments (2024 ICML)


### Introduction

So far, drug discovery has focused on identifying new molecules with desired properties within the vast chemical space. Among various approaches, fragment-based drug discovery (FBDD) has proven to be an effective strategy for exploring this space. Leveraging the strengths of FBDD, many molecule generative models have adopted fragment-based strategies to generate plausible and diverse molecules with target properties.

However, existing fragment extraction or motif mining methods suffer from two key limitations: (1)They do not consider the target chemical property in the context of drug discovery.(2) They rely on heuristic extraction rules, such as random sampling, frequency-based selection, or computationally expensive methods that produce substructures difficult to assemble.
To overcome these issues, the authors propose a deep learning-based, goal-aware fragment extraction method called Fragment-wise Graph Information Bottleneck (FGIB). This method uses the molecular structure and corresponding activity—referred to as the structure-activity relationship (SAR)—and applies the Graph Information Bottleneck (GIB) to identify substructures that are most relevant to the target property. The effectiveness of FGIB over other fragment extraction methods is illustrated in Figure 1(b). For assembling fragments into full molecules, the model employs a Soft Actor-Critic (SAC) agent as the assembly module and a Genetic Algorithm (GA) for the modification module. Furthermore, the framework allows extraction of new fragments during the generation process, dynamically updating the vocabulary and enhancing the model’s ability to generate novel and diverse molecules.

<img src="{{ 'assets/img/2025-04-28-20255142/geam_fig_1.png' | relative_url }}" alt="transformer" width="100%" class="l-body rounded z-depth-1 center">
<div class="l-gutter caption" markdown="1">
</div>


### Methodology

Proposed method Goal-aware fragment Extraction, Assembly, and Modification (GEAM) framework for molecule generation satisfying the target properties with well-extracted fragments by goal. The description about GEAM would be (1) goal-aware fragments, (2) fragments assembly method, and (3) fragment modification method, the dynamic vocabulary.



#### Goal-aware Fragment Extraction

The authors consider a dataset $\mathcal{D} = {(G_i, Y_i)}_{i=1}^N$, where each molecular graph $G_i = (X_i, A_i)$ consists of $n$ atoms, with node features and adjacency matrix defined as:

$X_i \in \mathbb{R}^{n \times d}, \quad A_i \in \mathbb{R}^{n \times n}$

Each graph is associated with a property:

$Y_i \in [0, 1]$

Let $\mathcal{G} = {G_i}_{i=1}^N$ denote the collection of graphs, and $V$, $E$ be the sets of all nodes and edges across $\mathcal{G}$.


The objective is to extract goal-aware fragments that are informative for predicting molecular properties while minimizing redundancy. To this end, the authors propose Fragment-wise Graph Information Bottleneck (FGIB), which learns to identify salient fragments for property prediction.

A set of candidate fragments is extracted from $\mathcal{G}$ using BRICS decomposition (Degen et al., 2008). Let $\mathcal{F}$ denote the resulting fragment set, where each fragment $F = (V, E) \in \mathcal{F}$ is a subgraph of a graph in $\mathcal{G}$. Any graph $G$ can be decomposed into $m$ fragments $\{F_j\}_{j=1}^m \subset \mathcal{F}$.

FGIB aims to find a subset of fragments $\mathcal{G}_{\text{sub}}$ that is maximally informative about the target property $Y$, while least informative about the original graph $G$:

$$
\min_{\mathcal{G}_{\text{sub}}} \; -I(\mathcal{G}_{\text{sub}}, Y) + \beta I(\mathcal{G}_{\text{sub}}, G)
$$

where $\beta > 0$ is a trade-off hyperparameter and $I(\cdot, \cdot)$ denotes mutual information.

The authors first compute node embeddings $\{h_i\}_{i=1}^n$ using a message passing neural network (MPNN):

$$
[h_1, \dots, h_n]^T = \text{MPNN}(X, A)
$$

Each fragment embedding $e_j \in \mathbb{R}^d$ is then computed by average pooling:

$$
e_j = \text{Avg}(\{ h_\ell \mid v_\ell \in V_j \})
$$

The importance of each fragment $F_j$ is predicted using an MLP with sigmoid activation:

$$
w_j = \text{MLP}(e_j), \quad w_j \in [0, 1]
$$

To control the flow of information, fragment embeddings are perturbed based on their importance scores:

$$
\tilde{e}_j = w_j e_j + (1 - w_j) \hat{\mu} + \epsilon, \quad \epsilon \sim \mathcal{N}(0, (1 - w_j)\hat{\Sigma})
$$

where $\hat{\mu} \in \mathbb{R}^d$ and $\hat{\Sigma} \in \mathbb{R}^{d \times d}$ are empirical statistics computed from $\{e_j\}_{j=1}^m$.

Let $Z = \text{vec}([\tilde{e}_1, \dots, \tilde{e}_m])$ be the concatenated latent variable. Assuming no information loss from fragment encoding, the objective becomes:

$$
\min_{\theta} \; -I(Z, Y; \theta) + \beta I(Z, G; \theta)
$$

Using variational inference (Alemi et al., 2017), the authors approximate the above objective with the following upper bound:

$$
\mathcal{L}(\theta, \phi) = \frac{1}{N} \sum_{i=1}^N \left[ -\log q_\phi(Y_i \mid Z_i) + \beta \, \text{KL}(p_\theta(Z \mid G_i) \| u(Z)) \right]
$$

where $q_\phi$ is a property predictor, $u(Z)$ is a variational Gaussian distribution, and $Z_i \sim \mathcal{N}(\mu_\theta(G_i), \Sigma_\theta(G_i))$.

After training, the authors compute a score for each fragment $F_j = (V_j, E_j) \in \mathcal{F}$ as:

$$
\text{score}(F_j) = \frac{1}{|S(F_j)|} \sum_{(G, Y) \in S(F_j)} \frac{w_j(G, F_j)}{\sqrt{|V_j|}} \cdot Y
$$

Here, $S(F_j) = \{(G, Y) \in \mathcal{D} \mid F_j \subseteq G\}$, and $w_j(G, F_j)$ is the fragment importance computed within graph $G$.

The normalization term \( \sqrt{\lvert V_j \rvert} \) is introduced to correct for fragment size effect. Finally, the top-\( K \) scoring fragments are selected as the goal-aware vocabulary \( \mathcal{S} \subset \mathcal{F} \) for downstream molecule generation.



#### Fragment Assembly

The next stage involves generating molecules using the goal-aware fragment vocabulary extracted earlier. To perform this task, the authors adopt a fragment assembly module based on Soft Actor-Critic (SAC), which learns to construct molecules with desirable properties through reinforcement learning (RL).

Following prior work, the fragment assembly process is framed as a sequential decision-making problem. Given a partially constructed molecule $g_t$, representing the state $s_t$ at time step $t$, the policy network selects a sequence of three actions to add a new fragment:

1. Choose an attachment site on $g_t$,
2. Select a fragment $F \in \mathcal{S}$ to attach,
3. Choose an attachment site on $F$.

To encode the current graph state, the nodes in $g_t$ are passed through a GCN (Kipf & Welling, 2017), yielding node embeddings $H = \text{GCN}(g_t)$, which are aggregated via sum pooling into the graph representation $h_{g_t} = \text{Sum}(H)$.

The policy $\pi$ consists of three sub-policies, each responsible for one of the sequential actions:

$$
\begin{aligned}
p_{\pi_1}(\cdot \mid s_t) &= \pi_1(Z_1), \quad Z_1 = f_1(h_{g_t}, H_{\text{att}}) \\\\
p_{\pi_2}(\cdot \mid a_1, s_t) &= \pi_2(Z_2), \quad Z_2 = f_2(z_{1,a_1}, \text{ECFP}(\mathcal{S})) \\\\
p_{\pi_3}(\cdot \mid a_{1:2}, s_t) &= \pi_3(Z_3), \quad Z_3 = f_3(\text{Sum}(\text{GCN}(F_{a_2})), H_{\text{att}}, F_{a_2})
\end{aligned}
$$

Here, $H_{\text{att}}$ represents node embeddings at attachment points, and $a_{1:2} = (a_1, a_2)$ denotes the selected actions. The authors utilize multiplicative interaction functions for $f_1, f_2, f_3$ to effectively combine heterogeneous input spaces.

The first policy $\pi_1$ selects an attachment site on $g_t$, the second policy $\pi_2$ chooses a fragment from the vocabulary $\mathcal{S}$, and the third policy $\pi_3$ determines where on the selected fragment to attach it. The new graph $g_{t+1}$ is obtained by joining the selected fragment $F_{a_2}$ to $g_t$ at the chosen sites $a_1$ and $a_3$. Repeating this procedure for $T$ steps yields a complete molecule $g_T = G$, which is then evaluated by an oracle to compute the reward $r_T$.

The SAC training objective used to optimize the policy $\pi$ is:

$$
\max_{\pi} \sum_t \mathbb{E}_{(s_t, a_t) \sim \rho_\pi} \left[ r(s_t, a_t) + \alpha \, \mathcal{H}(\pi(\cdot \mid s_t)) \right]
$$

where $\rho_\pi(s_t, a_t)$ is the state-action marginal distribution, $\mathcal{H}$ denotes the entropy of the policy, and $\alpha > 0$ is a temperature parameter encouraging exploration. The full policy is composed as:

$$
\pi(a_t \mid s_t) = p_{\pi_3}(a_{3,t} \mid a_{1:2,t}, s_t) \cdot p_{\pi_2}(a_{2,t} \mid a_{1,t}, s_t) \cdot p_{\pi_1}(a_{1,t} \mid s_t)
$$

To enable differentiable sampling of discrete actions, the authors apply the Gumbel-Softmax trick.
Assuming that the policy \( \pi \) performs at least as well as a random policy when choosing fragments, and is optimal for the attachment decisions conditioned on the fragment, the authors define \( \phi(\mathcal{S}) \) as the set of all optimal and novel molecules generable from fragment vocabulary \( \mathcal{S} \). Let \( \lvert \mathcal{S} \rvert^T \) denote the number of all possible fragment assembly trajectories of length \( T \).


The following upper bound is derived:

**Proposition 3.1**  
The probability that $\pi$ fails to generate at least one optimal molecule $G \in \phi(\mathcal{S})$ is at most:

$$
p = 
\begin{cases}
\left(1 + N \log \left( \frac{|\mathcal{S}|^T}{|\mathcal{S}|^T - \phi(\mathcal{S})} \right)\right)^{-1}, & \text{if } |\mathcal{S}|^T \ne \phi(\mathcal{S}) \\\\
0, & \text{if } |\mathcal{S}|^T = \phi(\mathcal{S})
\end{cases}
$$

This proposition implies that reducing the vocabulary size $|\mathcal{S}|$ or increasing the diversity $\phi(\mathcal{S})$ improves the success rate.

The authors note that the information bottleneck objective can be rewritten as:

$$
I(\mathcal{G}_{\text{sub}}, G) = I(\mathcal{G}_{\text{sub}}, Y) + I(\mathcal{G}_{\text{sub}}, G \mid Y)
$$

Thus, FGIB effectively maximizes \( \phi(\mathcal{S}) \) by increasing \( I(\mathcal{G}_{\text{sub}}, Y) \) and simultaneously reducing the vocabulary size \( \lvert \mathcal{S} \rvert \) through minimizing \( I(\mathcal{G}_{\text{sub}}, G \mid Y) \).


Compared to selecting a random subset $\mathcal{S}$ of the same size or using the full set of fragments from training data, FGIB produces a more compact and informative vocabulary. However, since $\phi(\mathcal{S})$ is still upper bounded, the authors propose a dynamic vocabulary update in the next section to further expand it.




#### Fragment Modification and Dynamic Vocabulary Update

The fragment assembly module alone is limited to producing molecules composed exclusively of fragments from a fixed, predefined vocabulary. This restriction impedes the generation of structurally diverse molecules and hinders exploration beyond the initial fragment set. To address this limitation, the authors introduce a fragment modification module based on a genetic algorithm (GA), designed to expand the fragment space through the creation of novel structures.

More specifically, the authors implement a graph-based genetic algorithm. In the initial round of the GA, the population is seeded with the top-$P$ molecules generated by the fragment assembly module. The algorithm then iteratively selects parent molecules and applies crossover and mutation operations to produce offspring. These genetic operations yield new molecules containing novel fragments that were not present in the original vocabulary.

In subsequent rounds, the top-$P$ molecules generated by both SAC-based assembly and GA-based modification are combined to form the next generation’s population. This iterative process alternates between fragment assembly and fragment modification.

The generative framework that cycles between these two modules—without further vocabulary updates—is referred to as **GEAM-static**.

To further promote chemical diversity and structural novelty, the authors integrate the fragment extraction module (FGIB) into this iterative pipeline. After each round of molecule generation by assembly and modification, FGIB is applied to extract new goal-aware fragments $\mathcal{S}'$ from the resulting molecules, following the procedure in Section 3.1.

The fragment vocabulary is then dynamically updated as:

$$
\mathcal{S} \leftarrow \mathcal{S} \cup \mathcal{S}'
$$

If the updated vocabulary exceeds the predefined maximum size $L$, the authors retain only the top-$L$ fragments according to the scoring function described in Equation (6). The updated vocabulary $\mathcal{S}$ is then used in the next iteration of fragment assembly.

This dynamic generative framework is referred to as **GEAM**, which continually refines its fragment vocabulary through interaction between assembly, modification, and extraction modules.


<img src="{{ 'assets/img/2025-04-28-20255142/geam_fig_2.png' | relative_url }}" alt="transformer" width="100%" class="l-body rounded z-depth-1 center">
<div class="l-gutter caption" markdown="1">
</div>


### Experiments
The authors validate the effectiveness of GEAM on two sets of multi-objective molecular optimization tasks that reflect real-world drug discovery scenarios. The first set of experiments focuses on generating molecules with high binding affinity, drug-likeness, and synthesizability. The second set is conducted on a widely used benchmark for practical molecular optimization. Additional ablation and qualitative analyses are also presented.

<img src="{{ 'assets/img/2025-04-28-20255142/geam_fig_3.png' | relative_url }}" alt="transformer" width="100%" class="l-body rounded z-depth-1 center">
<div class="l-gutter caption" markdown="1">
</div>

<img src="{{ 'assets/img/2025-04-28-20255142/geam_fig_4.png' | relative_url }}" alt="transformer" width="100%" class="l-body rounded z-depth-1 center">
<div class="l-gutter caption" markdown="1">
</div>
#### Optimization of Binding Affinity under QED, SA and Novelty Constraints

Following the setup of prior work, GEAM is tested on five docking score (DS) optimization tasks under constraints of drug-likeness (QED), synthetic accessibility, and novelty. The goal is to generate molecules that are novel, synthesizable, drug-like, and have high docking affinity. The target property \( Y(G) \) is defined as:

$$
Y(G) = \text{DS}(G) \times \text{QED}(G) \times \text{SA}(G) \in [0, 1]
$$

where DS and SA are normalized scores. The authors use the ZINC250k dataset  to train FGIB for predicting \( Y \) and extracting initial fragments.

To evaluate optimization performance, 3,000 molecules are generated and assessed using the following metrics:

- **Novel hit ratio (%):** The percentage of unique molecules that are both novel and meet the predefined hit criteria (DS lower than the median of known actives, QED > 0.5, SA < 5). A molecule is considered novel if its maximum Tanimoto similarity with any training molecule is below 0.4.
- **Novel top 5% DS (kcal/mol):** The average docking score of the top 5% of novel and unique hits.
- **Novelty (%):** The fraction of generated molecules not seen in training.
- **#Circles:** The number of distinct chemical scaffolds covered by the generated hits.

Protein targets used for docking score calculation include **parp1**, **fa7**, **5ht1b**, **braf**, and **jak2**. Experimental details are provided in Sections D.1 and D.2.

The authors compare GEAM against several baselines:

- **REINVENT** (Olivecrona et al., 2017): A SMILES-based RL model with a pretrained prior.
- **Graph GA** (Jensen, 2019): A GA-based model with fixed crossover and mutation rules.
- **MORLD** (Jeon & Kim, 2020): A reinforcement learning model based on the MolDQN algorithm.
- **HierVAE** (Jin et al., 2020a): A VAE-based method using hierarchical motif representations.
- **RationaleRL** (Jin et al., 2020b): An RL model that extends rationales (property-relevant subgraphs) into full molecules.
- **FREED** (Yang et al., 2021): A fragment-based RL model using CReM fragments.
- **PS-VAE** (Kong et al., 2022): A VAE-based model that uses mined principal subgraphs.
- **MOOD** (Lee et al., 2023b): A diffusion model that promotes out-of-distribution generation to improve novelty.

<img src="{{ 'assets/img/2025-04-28-20255142/geam_fig_5.png' | relative_url }}" alt="transformer" width="100%" class="l-body rounded z-depth-1 center">
<div class="l-gutter caption" markdown="1">
</div>

<img src="{{ 'assets/img/2025-04-28-20255142/geam_fig_6.png' | relative_url }}" alt="transformer" width="100%" class="l-body rounded z-depth-1 center">
<div class="l-gutter caption" markdown="1">
</div>

Experimental results in Tables 1 and 2 demonstrate that both GEAM and GEAM-static significantly outperform all baseline methods across all tasks. These findings confirm the effectiveness of combining goal-aware fragment extraction with fragment assembly via SAC and modification via GA. Among the two, GEAM (with dynamic vocabulary updates) often performs comparably or better than GEAM-static. As further supported by Tables 3 and 4, the inclusion of dynamic vocabulary updates leads to higher novelty and structural diversity without compromising optimization performance.


A commonly observed trend in molecular design is that more powerful models tend to generate less diverse outputs. However, GEAM circumvents this trade-off by continually discovering high-quality, goal-relevant fragments during generation. Notably, although MORLD reports high novelty scores, these are accompanied by poor optimization results and very limited diversity, making them less meaningful. Likewise, while RationaleRL achieves high diversity on targets like **5ht1b** and **jak2**, its weak optimization and novelty reduce the significance of such diversity values.



#### Optimization of Multi-property Objectives in PMO Benchmark
The authors evaluate GEAM on seven multi-property optimization (MPO) tasks from the Practical Molecular Optimization (PMO) benchmark. These tasks are drawn from the Guacamol benchmark, with added constraints on the number of oracle calls to better reflect real-world drug discovery settings.

For comparison, the authors adopt the top three models. Since prior work(Gao et al.)'s study includes a total of 25 baseline models, demonstrating superiority over the top three is considered representative of surpassing the full set. In addition to REINVENT and Graph GA, the authors include STONED, a genetic algorithm that operates on SELFIES representations.

<img src="{{ 'assets/img/2025-04-28-20255142/geam_fig_7.png' | relative_url }}" alt="transformer" width="100%" class="l-body rounded z-depth-1 center">
<div class="l-gutter caption" markdown="1">
</div>
As shown in Table 5, GEAM consistently outperforms these baselines on the majority of the PMO tasks, illustrating its robustness across a wide range of drug design challenges. Moreover, in several tasks, GEAM exhibits clear improvements over GEAM-static, demonstrating the effectiveness of dynamic vocabulary updates.

Further analysis in Table 6 highlights that GEAM achieves higher novelty and diversity compared to baseline models. In particular, GEAM generates more structurally novel and diverse molecules than GEAM-static, confirming that its dynamic vocabulary update mechanism enhances exploration without sacrificing property optimization.

#### Ablation Studies and Qualitative Analysis
To evaluate the effectiveness of the proposed goal-aware fragment extraction strategy using FGIB, the authors compare FREED against FREED (FGIB) in Figure 3(a). FREED (FGIB) is a variant where the original fragment vocabulary is replaced with fragments extracted via FGIB. The results show that FREED (FGIB) substantially outperforms the original, indicating that goal-aware fragment extraction with FGIB significantly enhances optimization performance.

<img src="{{ 'assets/img/2025-04-28-20255142/geam_fig_8.png' | relative_url }}" alt="transformer" width="100%" class="l-body rounded z-depth-1 center">
<div class="l-gutter caption" markdown="1">
</div>




The authors further compare GEAM to several ablated versions using alternative fragment vocabularies, as illustrated in Figure 3(b). These include:
- **GEAM (FREED):** using FREED’s fragment vocabulary,
- **GEAM (MiCaM):** using the MiCaM vocabulary,
- **GEAM (BRICS):** using randomly selected BRICS fragments,
- **GEAM (property):** scoring fragments solely based on the target property, without using Eq. (6), i.e.,

  $$
  \text{score}(F_j) = \frac{1}{|S(F_j)|} \sum_{(G,Y)\in S(F_j)} Y
  $$

Among all variants, GEAM achieves the highest performance, validating the importance of using FGIB-based goal-aware fragment scoring. Notably, even though GEAM (property) selects top fragments based purely on their property scores, its performance is inferior to GEAM, highlighting the benefit of FGIB in identifying substructures truly responsible for the desired properties.

To examine the benefit of combining fragment assembly and modification modules, the authors compare GEAM with ablated versions in Figure 3(c):
- **GEAM-w/o A:** skips the assembly module and selects top-$P$ molecules from ZINC250k as the population,
- **GEAM-w/o M:** omits the modification module,
- **GEAM-random A:** replaces SAC-based assembly with random fragment assembly.

GEAM-w/o A performs poorly, demonstrating that modification alone cannot leverage the expressiveness of goal-aware fragments. GEAM-random A shows significant improvement over GEAM-w/o A, but is still outperformed by GEAM. These results confirm that the synergy between assembly and modification modules is critical for high-performance molecule generation.

<img src="{{ 'assets/img/2025-04-28-20255142/geam_fig_9.png' | relative_url }}" alt="transformer" width="100%" class="l-body rounded z-depth-1 center">
<div class="l-gutter caption" markdown="1">
</div>
To further investigate the role of dynamic vocabulary updates, Figure 4 compares GEAM with GEAM-static and GEAM-static-1000. GEAM-static-1000 refers to a variant with a fixed vocabulary of size \(K = 1000\). In contrast, GEAM begins with \(K = 300\) and dynamically expands its vocabulary to a maximum of \(L = 1000\) as it collects novel fragments during generation. While GEAM-static-1000 achieves high diversity due to its large vocabulary, its optimization performance suffers, likely because it starts with a broader but less focused fragment pool.

GEAM outperforms all static variants in both optimization performance and novelty. This improvement is attributed to the dynamic integration of novel fragments beyond the training set, allowing the model to explore more promising chemical spaces without compromising structural quality.

The authors also perform a qualitative analysis of the extracted fragments. Figure 3(d) presents a case study involving the binding interaction between a generated molecule and the target protein **jak2**, analyzed using PLIP. Fragments are visualized along with their importance weights $w$ as computed by FGIB. Fragments with higher weights (highlighted in red and blue) are shown to directly participate in protein-ligand interactions, while those with lower weights (gray) are not involved.

This analysis provides strong evidence that FGIB successfully identifies functionally important fragments and supports its potential for improving interpretability in molecular design tasks. Additional case studies are shown in Figure 7.


### Takeaway



## [Paper 2] MAGNet: Motif-Agnostic Generation of Molecules From Scaffolds (2025 ICLR)

### Introduction

Generative models for novel compound generation have emerged as powerful tools in domains such as drug discovery and materials science. Many approaches incorporate molecular fragments (i.e., motifs) to explicitly encode core structures—such as rings—providing strong inductive biases during generation. However, including all possible motifs results in an unmanageably large vocabulary. To address this, most methods limit the vocabulary to the top-k most frequent motifs. While this alleviates the scalability issue, it forces the model to decode complex structures at the atom-to-atom level when uncommon motifs are needed. Although many generative models rely on a vocabulary of the most common molecular motifs, this approach fails to capture the full structural diversity of chemical space. As shown in Fig. 1a, even state-of-the-art models struggle to reconstruct key substructures of FDA-approved drugs when those motifs are missing from the vocabulary. This limitation raises concerns about their ability to generate truly novel drug-like molecules.
To address this, the authors propose abstracting motifs into more general scaffolds—featureless structural templates similar to Murcko scaffolds (Fig. 1b). This reduces vocabulary size while preserving structural diversity. Atom and bond types are then learned separately, enabling the generation of novel motifs beyond a fixed set. The authors introduce a new factorization of the molecular distribution, decomposing molecules into components that allow for a structurally rich yet compact scaffold vocabulary. They implement this with MAGNet, a hierarchical generative model that enables flexible sampling and feature learning. Moreover, they argue that existing benchmarks insufficiently assess structural diversity. To overcome this, they propose new evaluations focused on decoding rare molecular structures and measuring motif-level diversity and accuracy in atom/bond assignments.

### Preliminary
The authors present a general mathematical formulation of the proposed generative framework. Although this section is not critical for understanding the main concepts in the remainder of the work, the matching model is formally introduced later in the *Modeling* section.

A molecule \(G\) is represented as a graph consisting of nodes (atoms), edges (bonds), and their corresponding features. The authors factorize the probability distribution 
\( P(G)\) as illustrated in Figure 2, decoupling structural information from node and edge features:

$$
P(G) = P(G \mid G_S) \cdot P(G_S), \quad \text{since } P(G_S \mid G) = 1
$$

Here, \( G \) refers to the full atom-level graph, while \( G_S \) represents a scaffold-level abstraction that captures the coarse topology of the molecule. This abstraction is defined by the multiset of scaffolds \( S \) and their typed connectivity matrix \( A \in \mathbb{A}^{\lvert S \rvert \times \lvert S \rvert} \), such that \( G_S = (S, A) \). The probability over scaffolds is further factorized as:

\[
p(G_S) = p(S) \cdot p(A \mid S)
\]


$$
P(G_S) = P(A \mid S) \cdot P(S)
$$

Each scaffold node \( S \) can be expanded into a binary adjacency matrix over its \( s = \lvert S \rvert \) constituent atoms, i.e., \( S \in \{0, 1\}^{s \times s} \), and enriched with atom and bond types. This feature-augmented scaffold is referred to as a **typed motif** or subgraph \( M \subset G \), and a molecule is thus associated with a multiset of such motifs, denoted \( M \).

Connectivity between scaffolds in \( G_S \), represented by \( A_{kl} \ne 0 \), implies that two scaffolds share a **join node** at the atom level. The set of join nodes \( J \) is defined as:


$$
J = \{ j \mid j \in M_k \cap M_l,\; A_{kl} \ne 0,\; S_k, S_l \in S \}
$$

By treating \( A \) as a typed matrix—i.e., \( A \in \{0, C, N, \dots\} \)—the join nodes can implicitly convey atom type information, which is explored through ablation studies in Appendix D.7.

Not all atoms are included in the scaffold graph, in order to maintain a compact and structurally distinct vocabulary of scaffolds (see Section 4.1). In particular, the authors define the set of **leaf nodes** \( L \), which includes nodes \( l \) with degree \( d_l = 1 \) whose neighbors \( k \in \mathcal{N}_l \) have degree \( d_k = 3 \).

With this structure, the full molecular graph \( G \) is defined in terms of its motifs \( M \), join nodes \( J \), and leaf nodes \( L \), enabling the factorization of the probability distribution as:

\[
P(G) := P(L, J, M) = P(L \mid M, J) \cdot P(J \mid M, A) \cdot P(M \mid G_S) \cdot P(G_S)
\]

In this formulation, \( G_S \) serves as the high-level structural prior. Importantly, join nodes \( J \) are conditionally independent of \( S \) given the motifs \( M \), and leaf nodes \( L \) are conditionally independent of \( A \) given \( J \).




### Methodology
Building on the prior factorization, the authors present a method to derive a compact scaffold vocabulary and introduce MAGNet, a generative model that operates on molecular graphs. MAGNet bridges the gap between atom-level and fragment-level generation by using scaffolds as fundamental units, subsequently generating atom and bond attributes for each.

To promote compactness and diversity, the authors propose a fragmentation strategy. Initially, all leaf nodes \( L \) are removed from a molecule \( G \), following earlier approaches. This segmentation divides the molecule into cyclic and acyclic components. Unlike traditional methods that connect fragments via bonds, MAGNet represents fragment connections using shared atoms, defined as join nodes \( \nu \in J \).

To minimize the number of scaffolds, the remaining acyclic structures are further decomposed into “junctions,” defined as central atoms of degree three or four with their neighbors. This reduces vocabulary complexity and improves scaffold distinctiveness. Unlike data-driven approaches, this top-down method ensures structural consistency. In the final step, atom and bond types are stripped from fragments to allow one scaffold to represent multiple variations, unlike fragment-based methods that require different tokens for slight variations.

MAGNet is trained as a VAE, where the latent variable \( z \) captures semantic features of the input graph. The training objective maximizes the ELBO:

\[
\log P(G) \geq \mathbb{E}_{q(z \mid G)}[\log P(G \mid z)] - D_{\text{KL}}(q(z \mid G) \,\|\, p(z))
\]


$$
\mathcal{L} = \mathbb{E}_{z \sim Q}[\log P(G|z)] - \beta D_{\text{KL}}(Q(z|G) \| P(z)),
$$


with \( P(z) \sim \mathcal{N}(0, 1) \).

The encoder employs a graph transformer to embed molecular graphs, decomposing information into four components: the molecule \( G \), scaffold graph \( G_S \), join nodes \( J \), and leaf nodes \( L \). These components are aggregated and mapped to a latent graph embedding \( z_G \).

From \( z \), MAGNet first generates the scaffold multiset \( S \) via a transformer decoder, followed by scaffold connectivity \( A \). The conditional probability for scaffold edges is modeled as:

\[
p(A \mid S, z) = \prod_{(i, j) \in \text{Edges}} p(A_{ij} \mid S, z)
\]



$$
P(A_{ij} = t \mid S, z), \quad t \in \{0, C, N, \ldots\}.
$$

The scaffold-level loss is:

$$
\mathcal{L}_{GS} = \mathcal{L}_S + \mathcal{L}_A,
$$

where \( \mathcal{L}_S \) and \( \mathcal{L}_A \) are the categorical losses for the scaffold set and their connectivity.

MAGNet then predicts atom types \( M_a^i \) for each scaffold \( S_i \) and determines bond types \( M_b \). Since motif structure must support join node types from \( A \), the motifs satisfy:


$$
A_{kl} \in \bigcap_j M_a^k \cap M_a^l.
$$

Join nodes \( J \) are identified by predicting node pairs \( (p_i, p_j) \) with merge probability,



$$
J_{ij}^{(k, l)} = P(p_i \equiv p_j \mid M, A, z),
$$

resulting in a joint matrix \( J^{(k, l)} \in [0,1]^{\lvert V_k \rvert \times \lvert V_l \rvert} \).

Finally, leaf nodes \( L \) are predicted using a transformer decoder conditioned on the atom graph \( C \). The overall atom-level loss is:


$$
\mathcal{L}_G = \mathcal{L}_M + \mathcal{L}_J + \mathcal{L}_L.
$$
The total training loss becomes \( \mathcal{L}_G + \mathcal{L}_{GS} \).

However, the authors observed that KL regularization alone may cause over-pruning in the latent space. To address this, a post-hoc normalizing flow is applied using Conditional Flow Matching, specifically the minibatch optimal transport variant.




### Experiments

The authors compare MAGNet against various state-of-the-art molecular generation models across multiple aspects of the generation pipeline. First, they assess the reconstruction and sampling of scaffolds (S), showing that MAGNet captures a wider range of structural patterns compared to baselines. Next, they evaluate generative quality on standard benchmarks. They also analyze MAGNet's capacity to flexibly assign atom and bond structures (M), producing a broader set of motifs than fixed-vocabulary methods. Lastly, they demonstrate MAGNet's utility in downstream tasks such as goal-conditioned generation and flexible scaffold control.

For comparison, the authors select representative VAE-based models, including both one-shot (e.g., PS-VAE) and sequential (e.g., MoLeR) generation approaches, along with MiCaM—a fragment-based model with a large motif vocabulary. They also benchmark MAGNet against distribution-learning models like JTVAE, HierVAE, GraphAF, SMILES-LSTM, and CharVAE.

Two benchmarks are used: GuacaMol (for distribution alignment, novelty, and KL/FCD metrics) and MOSES (for internal diversity, QED, SA, and logP). All models are trained on ZINC. Additional datasets—QM9, GuacaMol, ChEMBL, and L1000—support further evaluation.

In scaffold decoding \( P(S \mid z) \), MAGNet shows strong performance, accurately reconstructing complex or uncommon scaffolds that baselines struggle with due to limited motif vocabularies. Fig. 3a shows that baseline methods fail to build complex structures from atomic-level units, whereas MAGNet generates full scaffolds using a compact vocabulary.


Scaffold sampling behavior is also evaluated. MAGNet more closely matches the scaffold distribution in the training set, particularly for rare structures. The authors compute sampling ratios and observe that PS-VAE, MoLeR, and MiCaM tend to oversample certain scaffold types. MAGNet, however, aligns well across both common and uncommon categories (Fig. 3c).

In GuacaMol and MOSES evaluations (Tab. 1), MAGNet achieves comparable or better scores than all graph-based baselines. Although FCD is commonly used, the authors argue that it doesn't reflect structural diversity. For example, high FCD can be achieved with limited scaffold diversity.

Fig. 4a–b shows that MAGNet covers a broader motif distribution space. It outperforms in Maximum Mean Discrepancy (MMD) and visualizes more diverse fingerprints for scaffold-derived motifs. Unlike baselines with fixed motif sets, MAGNet learns bond and atom assignments, enabling generalization to unseen motif configurations.

Zero-shot generalization further supports MAGNet’s strength—it surpasses the best baseline by up to 20% in similarity score on new datasets.

In motif ranking evaluation (Fig. 4c), MAGNet achieves top-1 or top-2 matches between generated and ground truth motifs for most scaffolds. The authors also test goal-directed generation (Tab. 2), where MAGNet performs competitively or better than MoLeR and MiCaM.

MAGNet's architecture also enables advanced scaffold-conditioning capabilities. Unlike models restricted to connected graph extensions, MAGNet can condition on multiple, even disconnected, scaffolds. It supports multi-level control—conditioning on full scaffolds or finer fragments (Fig. 5)—offering greater flexibility for molecular design.



### Takeaway



